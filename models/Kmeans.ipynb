{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3e6f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import sys \n",
    "sys.path.append(\"/scratch/izar/kapps/DEX-Cyclic-Arbitrage/\")\n",
    "from config.get import cfg\n",
    "\n",
    "# import standard packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e7885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(cfg[\"files\"][\"encoded_train_features\"])\n",
    "X_test = np.load(cfg[\"files\"][\"encoded_test_features\"])\n",
    "train_ids = np.load(cfg['files']['train_ids']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d350efe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "tX_train = scaler.transform(X_train)\n",
    "tX_test  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8399c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_max = 12\n",
    "\n",
    "silhouettes = []\n",
    "sse = []\n",
    "for k in range(2, k_max): # Try multiple k\n",
    "    print(k,end=\"\\r\")\n",
    "    # Cluster the data and assign the labels\n",
    "    kmeans =  KMeans(n_clusters=k, random_state=42)\n",
    "    labels =  kmeans.fit_predict(X_train)\n",
    "    # Get the Silhouette score\n",
    "    score = silhouette_score(X_train, labels)\n",
    "    silhouettes.append({\"k\": k, \"score\": score})\n",
    "    \n",
    "    sse.append({\"k\": k, \"sse\": kmeans.inertia_})\n",
    "    \n",
    "# Convert to dataframes\n",
    "silhouettes = pd.DataFrame(silhouettes)\n",
    "sse = pd.DataFrame(sse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd53777",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Silhouette method\", \"SSE method\"))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=silhouettes.k, y=silhouettes.score, ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=sse.k, y=sse.sse, ),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.update_xaxes(title_text=\"k\", range=(2, k_max), row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"k\", range=(2, k_max), row=1, col=2)\n",
    "\n",
    "fig.update_layout(height=500, width=1000, title_text=\"K-means: evaluation metrics for different k\", showlegend=False)\n",
    "fig.write_html(f\"{cfg['fig_dir']['clustering']}kmeans_k_metrics.html\",full_html=False, include_plotlyjs=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0e9be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appears to be the best k value\n",
    "k = 4\n",
    "\n",
    "kmeans =  KMeans(n_clusters=k, random_state=42)\n",
    "train_labels = kmeans.fit_predict(tX_train)\n",
    "test_labels  = kmeans.predict(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3a3724",
   "metadata": {},
   "outputs": [],
   "source": [
    "fX_train = pd.read_csv(cfg[\"files\"][\"features_train\"]).drop(columns=['Unnamed: 0'])\n",
    "fX_test  = pd.read_csv(cfg[\"files\"][\"features_test\"]).drop(columns=['Unnamed: 0'])\n",
    "fX_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9fadab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fX_train['cluster'] = train_labels\n",
    "fX_train.cluster = fX_train.cluster.apply(str) # make plot look nicer\n",
    "fX_test['cluster'] = test_labels\n",
    "fX_test.cluster = fX_test.cluster.apply(str) # make plot look nicer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a085efa9",
   "metadata": {},
   "source": [
    "## Clustering validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6987e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_char_cluster(data, col, title, callback = lambda fig: 0, train=True):\n",
    "    set_name = ('train' if train else 'test')\n",
    "    fig = px.bar(\n",
    "        data.reset_index(), \n",
    "        x='cluster', y=col, color='cluster',\n",
    "        title=f\"{title} ({set_name} set)\")\n",
    "    callback(fig)\n",
    "    fig.write_html(f\"{cfg['fig_dir']['clustering']}{title.replace(' ', '_')}_{set_name}_small.html\", full_html=False, include_plotlyjs=False)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2848db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nb_cycles_clusters(grouper, **kwargs):\n",
    "    bar_char_cluster(grouper.count(),\n",
    "        'cycle_id',\"Number of cycles per cluster\",**kwargs)\n",
    "    \n",
    "def profits_clusters(grouper, **kwargs):\n",
    "    bar_char_cluster(grouper.median(),\n",
    "        'profits',\"Profit per cluster\", **kwargs)\n",
    "    \n",
    "def profitability_clusters(grouper, **kwargs):\n",
    "    bar_char_cluster(grouper.mean(),\n",
    "        'profitability',\"Profitability of each cluster\",\n",
    "         lambda fig: fig.update_yaxes(range=(0.9, 1)), **kwargs)\n",
    "\n",
    "def median_token_clusters(grouper, **kwargs):\n",
    "    def weighted_avg(g):\n",
    "        return pd.concat([g.token1, g.token2, g.token3]).value_counts().median()\n",
    "    \n",
    "    bar_char_cluster(grouper.apply(weighted_avg),\n",
    "        0,\"Median of token distribution within each cluster\", **kwargs)\n",
    "                     \n",
    "from scipy.stats import entropy\n",
    "\n",
    "def entropy_clusters(grouper, **kwargs):\n",
    "    def weighted_avg(g):\n",
    "        return entropy(pd.concat([g.token1, g.token2, g.token3]).value_counts())\n",
    "        \n",
    "    bar_char_cluster(grouper.apply(weighted_avg),\n",
    "        0,\"Entropy of token distribution within each cluster\", **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930fc0f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline_metrics= [\n",
    "    nb_cycles_clusters,\n",
    "    profits_clusters,\n",
    "    profitability_clusters,\n",
    "    median_token_clusters,\n",
    "    entropy_clusters\n",
    "]\n",
    "\n",
    "\n",
    "train_grouper = cluster_profits = fX_train.groupby('cluster')\n",
    "\n",
    "[agg(train_grouper) for agg in pipeline_metrics]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd28863",
   "metadata": {},
   "source": [
    "## Validation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c417938",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_grouper = fX_test.groupby('cluster')\n",
    "[agg(test_grouper, train=False) for agg in pipeline_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebed157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754c0060",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
