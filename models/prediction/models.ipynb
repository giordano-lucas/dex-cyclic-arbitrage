{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "907f511f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-11 15:04:13.533014: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "import tensorflow as tf\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "import sys, os \n",
    "sys.path.append('/'.join(os.getcwd().split('/')[:4]))\n",
    "from config.get import cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8365a4be",
   "metadata": {},
   "source": [
    "# Loading the data\n",
    "Predictions will use embeddings produced by the previously selected embeddings model as features. The target variable of the prediction is the boolean value corresponding to the profitability of cycles. The embedding was shuffled when splitting the embedding model training data. Thus, one needs to rematch each embedding with the corresponding target using `cycle_id`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c32c2a69",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'encoded_train_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26455/4080525343.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'files'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoded_train_features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_test\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'files'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoded_test_features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# load ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'encoded_train_features'"
     ]
    }
   ],
   "source": [
    "# load the features\n",
    "X_train = np.load(cfg['files']['encoded_train_features'])\n",
    "X_test  = np.load(cfg['files']['encoded_test_features'])\n",
    "\n",
    "# load ids\n",
    "train_ids = np.load(cfg['files']['train_ids']).astype(int)\n",
    "test_ids = np.load(cfg['files']['test_ids']).astype(int)\n",
    "train_ids = pd.DataFrame({\"cycle_id\":train_ids})\n",
    "test_ids  = pd.DataFrame({\"cycle_id\":test_ids})\n",
    "\n",
    "\n",
    "target = pd.read_csv(cfg['files']['features'])\n",
    "\n",
    "y_train = train_ids.join(target,on=\"cycle_id\",lsuffix=\"_\").profitability\n",
    "y_test = test_ids.join(target,on=\"cycle_id\",lsuffix=\"_\").profitability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c7af913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9444980177863496\n"
     ]
    }
   ],
   "source": [
    "print(y_train.mean()) # imbalanced classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eba0f1",
   "metadata": {},
   "source": [
    "# Rescale the features\n",
    "* Embeddings are normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62413dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "tX_train = scaler.transform(X_train)\n",
    "tX_test  = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2addb79",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e45db8",
   "metadata": {},
   "source": [
    "## model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd8ef2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = LogisticRegressionCV(cv=5,Cs=np.logspace(-4,4,10),class_weight=\"balanced\",max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cad7cd5",
   "metadata": {},
   "source": [
    "## fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1415fcb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=array([1.00000000e-04, 7.74263683e-04, 5.99484250e-03, 4.64158883e-02,\n",
       "       3.59381366e-01, 2.78255940e+00, 2.15443469e+01, 1.66810054e+02,\n",
       "       1.29154967e+03, 1.00000000e+04]),\n",
       "                     class_weight='balanced', cv=5, max_iter=1000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_model.fit(tX_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bf78ca",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae8400ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion(tn, fp, fn, tp):\n",
    "    print(f\"True neg : {tn} | False pos : {fp} | False neg : {fn} | True pos : {tp}\")\n",
    "    print(tabulate([['True (real)',tp, fn], ['False (Real)',fp, tn]], headers=['\\\\', 'True (pred)' ,\" False (pred)\"], tablefmt='fancy_grid'))\n",
    "   \n",
    "def evaluate_model(model,test_set=tX_test):\n",
    "\n",
    "    pred = model.predict(test_set)>0.5\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test,pred).ravel()\n",
    "    print_confusion(tn, fp, fn, tp)\n",
    "    f1 = f1_score(y_test,pred)\n",
    "    print(f\"f1 score={f1:0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ca64a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True neg : 112 | False pos : 104 | False neg : 1544 | True pos : 2241\n",
      "╒══════════════╤═══════════════╤═════════════════╕\n",
      "│ \\            │   True (pred) │    False (pred) │\n",
      "╞══════════════╪═══════════════╪═════════════════╡\n",
      "│ True (real)  │          2241 │            1544 │\n",
      "├──────────────┼───────────────┼─────────────────┤\n",
      "│ False (Real) │           104 │             112 │\n",
      "╘══════════════╧═══════════════╧═════════════════╛\n",
      "f1 score=0.7312\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(logistic_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6614c3af",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55523cd8",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b273e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_parameters = {'kernel':('linear', 'rbf','poly'), 'C':np.logspace(-4,4,5)}\n",
    "svc = svm.SVC()\n",
    "svm_model = GridSearchCV(svc, svm_parameters,verbose=1,cv = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ee1706",
   "metadata": {},
   "source": [
    "## Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5ca5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 15 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "svm_model.fit(tX_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d0e8fa",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a17362",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(svm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab5e9e5",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8575372",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "This new prediction method works on the raw data (not embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71a6ac00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9333, 3600) (4001, 3600)\n"
     ]
    }
   ],
   "source": [
    "# load the features\n",
    "X_train_NN = np.load(cfg['files']['raw_train_features'])\n",
    "X_test_NN  = np.load(cfg['files']['raw_test_features'])\n",
    "\n",
    "\n",
    "X_train_NN = X_train_NN.reshape((len(X_train_NN),-1))\n",
    "X_test_NN = X_test_NN.reshape((len(X_test_NN),-1))\n",
    "print(X_train_NN.shape,X_test_NN.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b04035f",
   "metadata": {},
   "source": [
    "## Scaling the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "09af30fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_scaler = StandardScaler()\n",
    "NN_scaler.fit(X_train_NN)\n",
    "tX_train_NN = NN_scaler.transform(X_train_NN)\n",
    "tX_test_NN  = NN_scaler.transform(X_test_NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c50c23a",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "234d2c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = X_train_NN.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "580c432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-30 13:36:40.054474: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-12-30 13:36:40.054528: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (izar): /proc/driver/nvidia/version does not exist\n",
      "2021-12-30 13:36:40.055970: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "NN_model = tf.keras.Sequential()\n",
    "NN_model.add(tf.keras.Input(shape=d))\n",
    "NN_model.add(tf.keras.layers.Dense(100,activation=\"relu\"))\n",
    "NN_model.add(tf.keras.layers.Dense(50,activation=\"relu\"))\n",
    "NN_model.add(tf.keras.layers.Dense(1,activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14585ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               360100    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 365,201\n",
      "Trainable params: 365,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f6fa7",
   "metadata": {},
   "source": [
    "## Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7174ff2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "compute_class_weight() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_33897/2199289988.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m class_weights = class_weight.compute_class_weight('balanced',\n\u001b[1;32m      4\u001b[0m                                                  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                                  y_train)\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: compute_class_weight() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38428000",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "292/292 [==============================] - 3s 9ms/step - loss: 9267022648477483008.0000A: 3\n",
      "Epoch 2/10\n",
      "292/292 [==============================] - 2s 8ms/step - loss: 0.2147\n",
      "Epoch 3/10\n",
      "292/292 [==============================] - 2s 8ms/step - loss: 0.2145A: 1s - \n",
      "Epoch 4/10\n",
      "292/292 [==============================] - 2s 8ms/step - loss: 0.2145\n",
      "Epoch 5/10\n",
      "292/292 [==============================] - 2s 8ms/step - loss: 0.2145\n",
      "Epoch 6/10\n",
      "292/292 [==============================] - 2s 8ms/step - loss: 0.2145\n",
      "Epoch 7/10\n",
      "292/292 [==============================] - 2s 8ms/step - loss: 0.2145\n",
      "Epoch 8/10\n",
      "292/292 [==============================] - 2s 8ms/step - loss: 0.2145\n",
      "Epoch 9/10\n",
      "292/292 [==============================] - 2s 8ms/step - loss: 0.2145\n",
      "Epoch 10/10\n",
      "292/292 [==============================] - 2s 8ms/step - loss: 0.2145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b8c27edd850>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_model.compile(optimizer='sgd', loss='binary_crossentropy')\n",
    "# This builds the model for the first time:\n",
    "NN_model.fit(X_train_NN, y_train, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1887fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True neg : 0 | False pos : 216 | False neg : 0 | True pos : 3785\n",
      "╒══════════════╤═══════════════╤═════════════════╕\n",
      "│ \\            │   True (pred) │    False (pred) │\n",
      "╞══════════════╪═══════════════╪═════════════════╡\n",
      "│ True (real)  │          3785 │               0 │\n",
      "├──────────────┼───────────────┼─────────────────┤\n",
      "│ False (Real) │           216 │               0 │\n",
      "╘══════════════╧═══════════════╧═════════════════╛\n",
      "f1 score=0.9723\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(NN_model,test_set=X_test_NN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
